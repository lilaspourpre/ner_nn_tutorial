{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow & Named-entity recognition tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "09/04/2018\n",
    "\n",
    "Никишина Ирина (НИУ ВШЭ, ИСП РАН)\n",
    "\n",
    "в тьюториале содержатся материалы лекций Е. Черняк (https://github.com/echernyak/ML-for-compling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Requirements (из тех, которые явные и про которые я помню):\n",
    "\n",
    "* tensorflow\n",
    "* tqdm\n",
    "* nltk\n",
    "* pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Что такое нейронные сети и с чем их едят (кажется, вы все уже все знаете)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Сеть прямого распространения  для классификации текстов\n",
    "\n",
    "\n",
    "![title](img/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $x$ - входное векторное представление текста\n",
    "* $h$ – скрытые слои с нелинейными функциями активации\n",
    "* $y$ – выходы, как правило, один $y$ соответствует одной метке класса \n",
    "\n",
    "$NN_{MLP2}(x) = y$\n",
    "\n",
    "$h_1 = g^1(xW^1 + b^1)$\n",
    "\n",
    "$h_2 = g^2(h^1 W^2 + b^2)$\n",
    "\n",
    "$y = h^2 W^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Нелинейные функции активации\n",
    "\n",
    "![title](img/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### dropout-регуляризация\n",
    "\n",
    "$NN_{MLP2}(x) = y$\n",
    "\n",
    "$h_1 = g^1(xW^1 + b^1)$\n",
    "\n",
    "$m^1 ~ Bernouli(r^1)$\n",
    "\n",
    "$\\hat{h^1} = m^1 \\odot h^1$\n",
    "\n",
    "$h_2 = g^2(\\hat{h^1} W^2 + b^2)$\n",
    "\n",
    "$m^2 ~ Bernouli(r^2)$\n",
    "\n",
    "$\\hat{h^2} = m^2 \\odot h^2$\n",
    "\n",
    "$y =\\hat{h^2} W^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Векторное представление текста \n",
    "* BOW (bag of words) – разреженное (sparse) векторное представление текста\n",
    "* CBOW (continious bag of words)  – плотное (dense) векторное представление текста\n",
    "\n",
    "$w_i$ – слово, $d_{emb}$ – размерность эмбеддинга слова, $E_{[w_i]}$ = $\\textbf{w}_i$\n",
    "\n",
    "\n",
    "#### Padding\n",
    "Входные тексты имеют переменную длинну, что неудобно, поэтому предположим, что они все состоят из одинакового количества слов, только часть из этих слов – баластные символы pad/\n",
    "\n",
    "\n",
    "#### Неизвестные слова (OOV)\n",
    "Если в тестовом множестве встретилось неизвестное слово, то можно \n",
    "* заменить его на pad;\n",
    "* заменить его на unk.  Однако в обучающем множестве unk никогда не встречается, поэтому его нужно добавить в обучающее множество искусственным образом. \n",
    "\n",
    "\n",
    "#### Word dropout - регуляризация \n",
    "Заменяем каждое слово на unk с вероятностью $\\frac{\\alpha}{|V| + \\alpha}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Собственно, задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FactRuEval2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* devset   (122 texts,  ≈ 30940 tokens) \n",
    "* testset  (132 texts,  ≈ 59382 tokens)  \n",
    "\n",
    "file structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](img/s1.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](img/s2.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](img/s3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "В рамках тестового задания в ИСП РАН была раелизована noname система, позволяющая обрабатывать данные FactRuEval и производить sequence labeling используя различные модели машинного обучения (SVM, Multilayer perceptron, LSTM, biLSTM, CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import main\n",
    "from machine_learning.rnn import RNN\n",
    "from machine_learning.cnn import CNN\n",
    "from machine_learning.rnn_trainer import RNNTrainer\n",
    "from machine_learning.cnn_trainer import CNNTrainer\n",
    "from machine_learning.svm_model_trainer import SvmModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model_path -- путь к обученной fasstext модели\n",
    "\n",
    "window -- размер окна (для учета контекста)\n",
    "\n",
    "ngram_affixes -- размер аффиксов (длина префиксов и суффиксов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trainset_path = \"../data/devset\"\n",
    "testset_path = \"../data/testset\"\n",
    "output_path = \"../output\"\n",
    "model_path = \"../data/ru.bin\"\n",
    "window=2\n",
    "ngram_affixes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs are ready for training 2018-04-09 07:52:57.032337\n",
      "Docs are ready for testing 2018-04-09 07:53:28.844042\n"
     ]
    }
   ],
   "source": [
    "morph_analyzer = main.pymorphy2.MorphAnalyzer()\n",
    "output_path = main.os.path.join(output_path, main.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "#embedding_model = main.get_model_for_embeddings(model_path)\n",
    "#print('Model is ready')\n",
    "train_documents = main.get_documents_with_tags_from(trainset_path, morph_analyzer)\n",
    "print('Docs are ready for training', main.datetime.now())\n",
    "test_documents = main.get_documents_without_tags_from(testset_path, morph_analyzer)\n",
    "print('Docs are ready for testing', main.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](img/bilou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded 2018-04-09 07:53:34.038209\n",
      "tags = ['O', 'BPER', 'BLOC', 'BORG', 'IPER', 'ILOC', 'IORG', 'LPER', 'LLOC', 'LORG', 'UPER', 'ULOC', 'UORG']\n"
     ]
    }
   ],
   "source": [
    "feature = main.get_composite_feature(window, train_documents, ngram_affixes, None)\n",
    "print('Features loaded', main.datetime.now())\n",
    "tags = main.compute_tags()\n",
    "print(\"tags = {}\".format(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сверточные нейронные сети [Convolutional neural networks, CNN]\n",
    "\n",
    "* Заимствованы из области компьютерного зрения\n",
    "* Пик популярности пришелся на 2014 (до +10% аккуратности в задачах классификации), со временем были вытеснены рекуррентными нейронными сетями \n",
    "* Помогают справится  с проблемой переменной длины входов (CNN VS window-based NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Слой свертки\n",
    "\n",
    "#### Фильтр [filter]:\n",
    "* $w_{1,n}$ – последовательность слов, $k$  – размер окна\n",
    "* $w_i$ , $d_{emb}$ – размерность эмбеддинга слова,  $\\textbf{w}_i \\in \\mathbb{R}^{d_{emb}} $\n",
    "* $\\textbf{x}_i = [\\textbf{w}_{i}, \\textbf{w}_{i+1}, \\ldots, \\textbf{w}_{i+k-1}]$, $\\textbf{x}_i \\in \\mathbb{R}^{k d_{emb}}$\n",
    "\n",
    "Фильтр: $p_i = g(\\textbf{x}_i  u)$, $p_i \\in \\mathbb{R}$, $u \\in \\mathbb{R}^{k d_{emb}}$\n",
    "\n",
    "\n",
    "![title](img/cnn1.png)\n",
    "\n",
    "\n",
    "Преобразуем каждое входное окно, но пока размерность входа не уменьшается!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Слой субдискретизации (пулинга, [pooling])\n",
    "\n",
    "* $h_i$ – выходные значения фильтра\n",
    "\n",
    "$\\max$-пулинг:\t$c = \\max_i h_i$\n",
    "\n",
    "\n",
    "![title](img/cnn2.png)\n",
    "\n",
    "* Выбираем самый важный признак из полученных на предыдущем шаге \n",
    "* Можем использовать и $\\min$, и усреднение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Многомерные фильтры\n",
    "\n",
    "Используем $\\textit{l}$ разных фильтров: $u_{1}, \\ldots, u_{\\textit{l}}$: \n",
    "\n",
    "$\\textbf{x}_i = [\\textbf{w}_{i}, \\textbf{w}_{i+1}, \\ldots, \\textbf{w}_{i+k-1}]$\n",
    "\n",
    "$\\textbf{p}_i = g(\\textbf{x}_i \\cdot  U+b)$\n",
    "\n",
    "$\\textbf{p}_i \\in \\mathbb{R}^{\\textit{l}} $, $\\textbf{x}_i \\in \\mathbb{R}^{k d_{emb}}$, $U \\in \\mathbb{R}^{k d_{emb} \\times \\textit{l}}$, $b \\in \\mathbb{R}^{\\textit{l}} $\n",
    "\n",
    "![title](img/cnn4.png)\n",
    "\n",
    "\n",
    "$\\max$-пулинг:\t$c_j = \\max_i h_{i,j}, j \\in [0,\\textit{l}]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](img/cnn6.png)\n",
    "\n",
    "Kim Y. Convolutional Neural Networks for Sentence Classification. 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:282: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(input_size=int(feature.get_vector_size()), output_size=len(tags), hidden_size=100, batch_size=8)\n",
    "model_trainer = CNNTrainer(epoch=1, nn=cnn, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCNN():\n",
    "    def __init__(self, input_size, output_size, hidden_size, batch_size, filter_sizes=(3, 4, 5)):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "\n",
    "        #self.x = \n",
    "        #self.y = \n",
    "        #self.seqlen = \n",
    "\n",
    "        output_layer = self.__add_convolution_layer(self.x, self.input_size, self.hidden_size)\n",
    "\n",
    "        self.outputs = tf.layers.dense() #???\n",
    "\n",
    "        self.loss = self.__add_loss(self.outputs)\n",
    "        self.train = self.__add_update_step(self.loss)\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def __add_convolution_layer(self, input_tensor, input_size, output_size):\n",
    "    input_tensor_expand = tf.expand_dims(input_tensor, -1)\n",
    "    conv_outputs = []\n",
    "\n",
    "    for i in range(len(self.filter_sizes)):\n",
    "        filter_size = self.filter_sizes[i]\n",
    "        conv_1 = self.__apply_convolution(filter_size, input_tensor_expand, input_size, output_size)\n",
    "        conv_2 = self.__apply_convolution(filter_size, input_tensor_expand, input_size, output_size)\n",
    "        result = conv_1 * tf.sigmoid(conv_2)\n",
    "        #max_pool_outputs = tf.nn.max_pool(result, [1, filter_size, 1, 1], [1, 1, 1, 1], 'SAME')\n",
    "        conv_outputs.append(result)\n",
    "\n",
    "    conv_out = tf.concat(conv_outputs, ?)\n",
    "    conv_out = tf.squeeze(conv_out, [?])\n",
    "    return conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def __apply_convolution(self, filter_size, input_tensor, input_size, output_size):\n",
    "        filter = tf.Variable(tf.random_normal([?, ?, 1, ?], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[?]), name=\"b\")\n",
    "        conv_outputs = tf.nn.conv2d(?, filter, [1, 1, ?, 1], 'SAME')\n",
    "        conv_outputs = tf.tanh(tf.nn.bias_add(conv_outputs, bias))\n",
    "        return conv_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __add_loss(self, logits):\n",
    "        #mask = \n",
    "\n",
    "        #losses = \n",
    "\n",
    "        #loss = \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def __add_update_step(self, loss):\n",
    "\n",
    "    #optimizer = \n",
    "    #update_step = \n",
    "    return update_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "custom_cnn = CustomCNN(input_size=int(feature.get_vector_size()), output_size=len(tags), hidden_size=100, batch_size=8)\n",
    "custom_model_trainer = CNNTrainer(epoch=5, nn=custom_cnn, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are created 2018-04-09 07:59:41.942645\n",
      "30940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.00011640788, batch: 195, epoch: 0: 100%|██████████| 196/196 [01:56<00:00,  1.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss: 0.0002458325  epoch: 0 \n",
      "Training finished 2018-04-09 08:01:39.283275\n"
     ]
    }
   ],
   "source": [
    "main.train_and_compute_nes_from(model_trainer=model_trainer, feature=feature, train_documents=train_documents,\n",
    "                               test_documents=test_documents, output_path=output_path)\n",
    "print(\"Testing finished\", main.datetime.now())\n",
    "print('Output path: \\n {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Рекуррентные нейронные языковые модели\n",
    "\n",
    "RNN позволяют уйти от Марковских допущений и позволяют учитывать предысторию произвольной длины.\n",
    "\n",
    "$x_{1:n} = x_1, x_2, \\ldots, x_n$, $x_i \\in \\mathbb{R}^{d_{in}}$\n",
    "\n",
    "$y_n = RNN(x_{1:n})$, $y_n \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "Для каждого префикса $x_{i:i}$ $y_i$ – выходной вектор.\n",
    "\n",
    "$y_i = RNN(x_{1:i})$\n",
    "\n",
    "$y_{1:n} = RNN^{*}(x_{1:n})$, $y_i \\in \\mathbb{R}^{d_{out}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![rnn](img/rnn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Управляемые архитектуры\n",
    "\n",
    "RNN трудно обучать: проблема исчезающего градиента. Уйти от нее помогают управляемые нейроны специального вида: LSTM и GRU.\n",
    "\n",
    "$s_i$ – память нейронной сети. Каждое использование $R$ считывает и видоизменяет всю память. \n",
    "\n",
    "Управляемый доступ к памяти: $g \\in {0,1}^n$:\n",
    "\n",
    "$s_{i+1} \\leftarrow g \\odot x + (1-g) \\odot s_{i}$\n",
    "\n",
    "Дифференцируемое управление:\n",
    "\n",
    "$g \\in \\mathbb{R}^n $:\n",
    "\n",
    "$s_{i+1} \\leftarrow \\sigma(g) \\odot x + (1-g) \\odot s_{i}$\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title1](img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/bilstm.png)\n",
    "author: A. Sysoev (Wikification encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/irina/ner_nn_tutorial/src/machine_learning/rnn.py:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('scope', reuse = tf.AUTO_REUSE ):\n",
    "    rnn = RNN(input_size=int(feature.get_vector_size()), output_size=len(tags), hidden_size=100, batch_size=8, bilstm=True)\n",
    "    bilstm_trainer = RNNTrainer(epoch=100, nn=rnn, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM():\n",
    "    def __init__(self, input_size, output_size, hidden_size, batch_size, bilstm=False):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.x = \n",
    "        self.y = \n",
    "        self.seqlen =\n",
    "        \n",
    "        self.fw_cell = tf.contrib.rnn.BasicLSTMCell(?)\n",
    "        self.bw_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "        self.mid_outputs, state = tf.nn.bidirectional_dynamic_rnn(self.fw_cell, self.bw_cell, self.x,\n",
    "                                                                      sequence_length=self.seqlen,\n",
    "                                                                      dtype=tf.float32)\n",
    "        self.mid_outputs = tf.concat(self.mid_outputs, 2)\n",
    "        \n",
    "        self.outputs = tf.contrib.layers.fully_connected(self.mid_outputs, size?, activation_fn=?)\n",
    "        \n",
    "        self.cross_entropy = \n",
    "        self.train = \n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are created 2018-04-09 03:36:14.813298\n",
      "30940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.25211728, batch: 13, epoch: 0:   7%|▋         | 13/196 [00:23<05:26,  1.79s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d1705601a6f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m main.train_and_compute_nes_from(model_trainer=bilstm_trainer, feature=feature, train_documents=train_documents,\n\u001b[0;32m----> 2\u001b[0;31m                                test_documents=test_documents, output_path=output_path)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing finished\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output path: \\n {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ner_nn_tutorial/src/main.py\u001b[0m in \u001b[0;36mtrain_and_compute_nes_from\u001b[0;34m(model_trainer, feature, train_documents, test_documents, output_path)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mne_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_nes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ner_nn_tutorial/src/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_trainer, feature, documents)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vectors are created'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tagged_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tagged_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_lenghts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ner_nn_tutorial/src/machine_learning/rnn_trainer.py\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(self, tagged_vectors, division)\u001b[0m\n\u001b[1;32m     19\u001b[0m         splitted_tags, _ = format_data(\n\u001b[1;32m     20\u001b[0m             self.translator([tagged_vector.get_tag() for tagged_vector in tagged_vectors], self.__tags), division)\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitted_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_of_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_of_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ner_nn_tutorial/src/machine_learning/rnn_trainer.py\u001b[0m in \u001b[0;36mrun_nn\u001b[0;34m(self, array_of_vectors, array_of_tags, seqlen_list)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                              {self.__nn.x: array_x,\n\u001b[1;32m     36\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseqlen_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                               })\n\u001b[1;32m     39\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main.train_and_compute_nes_from(model_trainer=bilstm_trainer, feature=feature, train_documents=train_documents,\n",
    "                               test_documents=test_documents, output_path=output_path)\n",
    "print(\"Testing finished\", datetime.now())\n",
    "print('Output path: \\n {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Спасибо за внимание. Ура!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
